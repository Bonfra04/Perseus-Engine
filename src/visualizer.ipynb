{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW METHOD\n",
    "# curl -L \"https://drive.usercontent.google.com/download?id=1q5Yp_kNA7gClJyFbOMxRkm12p9Qyjc60&confirm=xxx\" -o filename\n",
    "# https://drive.google.com/file/d/1q5Yp_kNA7gClJyFbOMxRkm12p9Qyjc60/view?usp=sharing\n",
    "## END NEW METHOD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# The following is an implementation of a tuning algorithm for Perseus's HCE. Perseus is a open-source chess engine that is written in C++.\n",
    "# We will get an already exported .bin file containing features extracted from the engine HCE. This way, the optimization problem becomes a simple logistic regression problem.\n",
    "# The only thing we need to account for is that the linear regression will be done with a slightly modified version of the sigmoid function (with a much larger range),\n",
    "# since the engine values are expected to vary between -1000 and 1000.\n",
    "# We will use torch to implement the logistic regression and the optimization algorithm.\n",
    "# The optimization algorithm will be the AdamW, and we will use weight decay (with a rather small value not to mess the sigmoid scale) to prevent overfitting.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Open the file containing the features. \n",
    "ENTRY_BYTE_SIZE = 500\n",
    "NUM_FEATURES = ENTRY_BYTE_SIZE - 1 # The last byte is the label.\n",
    "NUM_COLORLESS = ENTRY_BYTE_SIZE - 1 - 1 - 20 # 20 is the number of features that are color dependent, 1 is the phase feature, and 1 is the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7082116, 1])\n",
      "torch.Size([7082116, 478])\n",
      "torch.Size([7082116, 20])\n",
      "torch.Size([7082116, 1])\n",
      "torch.Size([71537, 1])\n",
      "torch.Size([71537, 478])\n",
      "torch.Size([71537, 20])\n",
      "torch.Size([71537, 1])\n"
     ]
    }
   ],
   "source": [
    "with open('out.bin', 'rb') as f:\n",
    "    features = f.read()\n",
    "    features = np.frombuffer(features, dtype=np.int8)\n",
    "# Reshape the features to have the correct shape. Do it not as a view, but as a copy.\n",
    "features = features.reshape(-1, ENTRY_BYTE_SIZE).copy()\n",
    "\n",
    "# Shuffle the features.\n",
    "np.random.seed(47) # 47 my beloved.\n",
    "np.random.shuffle(features)\n",
    "\n",
    "torch.manual_seed(47) # 47 my sweethearth.\n",
    "\n",
    "# Split the features. The first byte is the phase, then 478 colorless weights, followed by 20 color dependent features, and the last byte is the label.\n",
    "phases = features[:, 0]\n",
    "colorless = features[:, 1:NUM_COLORLESS + 1]\n",
    "color_dependent = features[:, NUM_COLORLESS + 1:NUM_COLORLESS + 1 + 20]\n",
    "labels = features[:, -1]\n",
    "\n",
    "\n",
    "# Divide the features into training and testing sets.\n",
    "train_size = int(0.99 * len(features))\n",
    "phases_train = phases[:train_size]\n",
    "colorless_train = colorless[:train_size]\n",
    "color_dependent_train = color_dependent[:train_size]\n",
    "labels_train = labels[:train_size]\n",
    "\n",
    "phases_test = phases[train_size:]\n",
    "colorless_test = colorless[train_size:]\n",
    "color_dependent_test = color_dependent[train_size:]\n",
    "labels_test = labels[train_size:]\n",
    "\n",
    "# Convert the numpy arrays to torch tensors.\n",
    "phases_train = torch.tensor(phases_train, dtype=torch.float32).unsqueeze(1)\n",
    "colorless_train = torch.tensor(colorless_train, dtype=torch.float32)\n",
    "color_dependent_train = torch.tensor(color_dependent_train, dtype=torch.float32)\n",
    "labels_train = torch.tensor(labels_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "phases_test = torch.tensor(phases_test, dtype=torch.float32).unsqueeze(1)\n",
    "colorless_test = torch.tensor(colorless_test, dtype=torch.float32)\n",
    "color_dependent_test = torch.tensor(color_dependent_test, dtype=torch.float32)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(phases_train.shape)\n",
    "print(colorless_train.shape)\n",
    "print(color_dependent_train.shape)\n",
    "print(labels_train.shape)\n",
    "\n",
    "print(phases_test.shape)\n",
    "print(colorless_test.shape)\n",
    "print(color_dependent_test.shape)\n",
    "print(labels_test.shape)\n",
    "\n",
    "# Delete the numpy array to free up memory.\n",
    "del features\n",
    "del phases\n",
    "del colorless\n",
    "del color_dependent\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ChessDataset(data.Dataset):\n",
    "    def __init__(self, phases, colorless, color_dependent, labels):\n",
    "        self.phases = phases\n",
    "        self.colorless = colorless\n",
    "        self.color_dependent = color_dependent\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.phases[idx], self.colorless[idx], self.color_dependent[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultcolorless = [\n",
    "    130, 432, 447, 623, 1369,\n",
    "\n",
    "    -1,    6,    6,    3,    3,    0,    3,    7 ,\n",
    "      36,   37,   22,   35,   23,   39,   15,   -3 ,\n",
    "       3,    1,    7,   11,   24,   12,    3,   -5 ,\n",
    "      -1,   -3,    0,    4,    6,    1,   -4,   -1 ,\n",
    "      -9,  -11,   -6,    2,    3,    0,   -7,   -1 ,\n",
    "      -9,   -7,   -9,  -11,   -9,  -16,    8,    3 ,\n",
    "     -43,  -43,  -28,  -33,  -27,  -17,    9,  -11 ,\n",
    "      -4,    2,    1,    1,   -1,   -1,   -8,    3 ,\n",
    "\n",
    "      -158,  -92,  -59,  -10,   23,  -45,  -53,  -85 ,\n",
    "      14,   43,   72,  102,   70,  167,   38,   59 ,\n",
    "       5,   51,   66,   94,  164,  151,   97,   58 ,\n",
    "      15,   21,   50,   91,   58,   94,   46,   84 ,\n",
    "      -6,    5,   18,   28,   47,   39,   58,   30 ,\n",
    "     -43,  -19,   -6,    1,   17,    7,   27,    9 ,\n",
    "     -79,  -62,  -42,  -28,  -12,  -25,   -6,   -4 ,\n",
    "     -77,  -35,  -42,  -10,   -3,    0,  -30,  -40 ,\n",
    "\n",
    "     -17,  -40,  -47,  -83,  -63,  -60,  -13,  -35 ,\n",
    "      18,   50,   28,   29,   60,   57,   34,    1 ,\n",
    "       1,   30,   36,   66,   63,  112,   70,   56 ,\n",
    "      -5,   11,   36,   70,   61,   52,   22,   12 ,\n",
    "      -4,   -4,    7,   44,   45,    6,   10,   39 ,\n",
    "       4,   35,   14,   12,    7,   19,   52,   35 ,\n",
    "      -9,  -20,   22,  -31,   -5,    3,   39,   46 ,\n",
    "       8,   47,    9,   -4,    0,    4,   22,   24 ,\n",
    "\n",
    "       20,    8,    4,    5,   24,   46,   51,   71 ,\n",
    "      49,   45,   54,   87,   67,  110,   83,  115 ,\n",
    "       0,   35,   30,   37,   97,  102,  133,  112 ,\n",
    "      -3,   11,    8,   14,   17,   28,   49,   60 ,\n",
    "     -25,  -26,  -22,  -17,  -17,  -30,   26,   24 ,\n",
    "     -30,  -23,  -28,  -32,  -23,  -16,   58,   27 ,\n",
    "     -65,  -58,  -31,  -39,  -23,  -23,   26,  -13 ,\n",
    "     -12,   -8,   -8,    0,   12,   12,   16,   -5 ,\n",
    "\n",
    "     -20,  -17,   14,   44,   48,   68,   73,   35 ,\n",
    "      42,    2,   11,   27,   38,   82,   52,  115 ,\n",
    "      17,   14,    6,   30,   51,  133,  118,  130 ,\n",
    "       4,   -2,    6,    3,    5,   37,   46,   56 ,\n",
    "       0,   -9,   -9,   -3,    4,    2,   24,   48 ,\n",
    "       9,   14,    0,  -13,  -15,    8,   45,   49 ,\n",
    "     -18,  -23,    2,   -5,   -1,   -2,   28,   54 ,\n",
    "       2,   19,   21,   23,   17,   -3,   17,   12 ,\n",
    "\n",
    "      17,   43,   41,    5,   17,   45,   81,   46 ,\n",
    "      56,  108,   63,   97,   83,  105,  137,  100 ,\n",
    "      23,  107,   53,   38,   71,  130,  125,   89 ,\n",
    "      31,   41,   12,  -33,  -26,   19,   43,    3 ,\n",
    "      19,   20,  -24,  -68,  -61,  -20,  -19,  -50 ,\n",
    "      49,   77,   -1,  -25,  -24,  -20,   44,   24 ,\n",
    "     130,   67,   68,   16,   21,   33,  128,  145 ,\n",
    "     152,  204,  171,   31,  121,   74,  186,  175 ,\n",
    "\n",
    "     -45, -10, 12, 26, 40, 50, 62, 75, 84,\n",
    "     -61, -14, 7, 19, 39, 50, 59, 64, 67, 72, 84, 92, 96, 81,\n",
    "     -10, -8, -3, 0, 6, 6, 8, 14, 20, 35, 37, 41, 46, 57, 47,\n",
    "     -20, -109, -46, -16, 2, 7, 14, 20, 27, 34, 41, 48, 52, 59, 62, 69, 71, 72, 78, 96, 101, 125, 136, 140, 151, 141, 89, 73, \n",
    "\n",
    "     20, 21, 14, 13, 9, 4, 2, 1, \n",
    "     0, 2, -20, -20, 20, 62, 79,\n",
    "     -4, 30, 25, 13, 45, 30, 21, 35,\n",
    "\n",
    "     # EG\n",
    "     153, 437, 452, 773, 1379,\n",
    "\n",
    "     -1,   -8,   -4,   -4,   -2,   -1,   -3,   -6 ,\n",
    "      53,   50,   46,   40,   43,   39,   44,   54 ,\n",
    "      30,   29,   25,   17,   16,   19,   23,   25 ,\n",
    "      16,   14,    3,   -4,   -6,    0,    6,    6 ,\n",
    "       6,    6,   -6,  -11,   -9,   -7,   -1,   -4 ,\n",
    "     -21,  -24,  -26,  -22,  -18,  -16,  -24,  -28 ,\n",
    "     -41,  -44,  -39,  -33,  -35,  -37,  -48,  -56 ,\n",
    "      -1,   -7,   -4,   -5,   -1,   -2,    2,   -5 ,\n",
    "\n",
    "      -61,  -19,    5,  -10,   -6,  -36,  -27,  -84 ,\n",
    "      52,   61,   43,   37,   22,    9,   43,   19 ,\n",
    "      33,   33,   61,   50,   23,   14,    7,    6 ,\n",
    "      23,   28,   51,   49,   49,   42,   26,    4 ,\n",
    "      20,   14,   43,   44,   50,   32,   10,   12 ,\n",
    "     -27,  -21,  -12,   18,   14,  -11,  -23,  -19 ,\n",
    "     -50,  -43,  -38,  -28,  -36,  -37,  -50,  -40 ,\n",
    "     -14,  -10,   -1,   10,   10,   -7,   -4,    6 ,\n",
    "\n",
    "     19,   26,   21,   26,   20,    7,   11,    2 ,\n",
    "      57,   62,   60,   49,   35,   44,   55,   49 ,\n",
    "      56,   47,   48,   25,   24,   32,   33,   40 ,\n",
    "      28,   32,   26,   41,   27,   29,   21,   22 ,\n",
    "       8,   21,   33,   31,   28,   24,   15,  -10 ,\n",
    "     -15,   -4,    2,   13,   24,    8,  -16,  -19 ,\n",
    "     -33,  -47,  -51,  -15,  -23,  -30,  -33,  -57 ,\n",
    "       3,   15,   16,   14,   16,   28,    2,  -23 ,\n",
    "\n",
    "        59,   61,   71,   61,   51,   54,   50,   43 ,\n",
    "     109,  124,  117,   93,   95,   86,   93,   82 ,\n",
    "      88,   85,   86,   71,   53,   48,   50,   41 ,\n",
    "      69,   61,   70,   59,   42,   43,   41,   33 ,\n",
    "      49,   48,   46,   41,   42,   44,   25,   20 ,\n",
    "      17,   12,    8,   15,    7,    3,  -30,  -20 ,\n",
    "     -16,  -14,  -13,   -9,  -23,  -26,  -43,  -41 ,\n",
    "      35,   25,   33,   20,   11,   22,   12,   18 ,\n",
    "\n",
    "       54,   64,   84,   77,   85,   73,   38,   54 ,\n",
    "     106,  148,  161,  166,  182,  144,  133,  130 ,\n",
    "      89,   96,  143,  144,  151,  131,   96,  102 ,\n",
    "      73,   91,  101,  130,  149,  132,  131,  115 ,\n",
    "      51,   80,   83,  115,  112,  110,   94,   88 ,\n",
    "       4,   31,   47,   62,   75,   69,   41,   25 ,\n",
    "     -28,  -25,  -18,    0,    1,  -21,  -48,  -63 ,\n",
    "       7,    5,    8,   27,   18,   15,    1,   -1 ,\n",
    "\n",
    "       -41,   15,   27,   56,   48,   61,   57,  -38 ,\n",
    "      96,  143,  140,  128,  146,  160,  155,  119 ,\n",
    "      88,  129,  145,  148,  156,  151,  148,  100 ,\n",
    "      56,  102,  130,  140,  141,  138,  117,   78 ,\n",
    "      31,   76,  109,  130,  131,  112,   92,   65 ,\n",
    "       1,   34,   63,   86,   87,   77,   47,   26 ,\n",
    "     -43,   -5,   12,   34,   38,   27,   -5,  -47 ,\n",
    "     -43,  -24,    3,   34,   10,   28,  -15,  -63 ,\n",
    "\n",
    "     -76, -5, 31, 45, 57, 73, 71, 68, 52,\n",
    "     -48, -32, -1, 25, 40, 56, 66, 70, 76, 76, 69, 67, 63, 56,\n",
    "     -90, 19, 41, 61, 70, 83, 91, 92, 98, 103, 108, 112, 117, 110, 114,\n",
    "     -12, -78, -69, -51, -19, 9, 48, 78, 98, 107, 118, 128, 140, 149, 158, 159, 175, 181, 184, 179, 184, 170, 165, 158, 156, 145, 116, 105,\n",
    "\n",
    "    48, 17, 7, 29, 7, 21, 2, 1,\n",
    "    0, -34, -21, 18, 66, 171, 263,\n",
    "    8, -2, 9, 4, 97, -8, 20, 30\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 956])\n"
     ]
    }
   ],
   "source": [
    "default = torch.tensor(default, dtype=torch.float32).view(1, -1)\n",
    "print(default.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 956])\n",
      "torch.Size([1, 478])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_884116/1937539170.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ptfile))\n"
     ]
    }
   ],
   "source": [
    "###### Now we will read the default values for the features. These values are the ones that the engine uses when no tuning is applied.\n",
    "# with open('default.bin', 'rb') as f:\n",
    "#     default = f.read()\n",
    "#     default = np.frombuffer(default, dtype=np.int8)\n",
    "#     default = default.reshape(-1, NUM_FEATURES)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to torch tensor.\n",
    "# default = torch.zeros((NUM_FEATURES-1) * 2) #torch.tensor(default, dtype=torch.float32)\n",
    "\n",
    "def load_model(ptfile):\n",
    "    model = HCE()\n",
    "    model.load_state_dict(torch.load(ptfile))\n",
    "    return model\n",
    "\n",
    "class BoundedPolynomial(nn.Module):\n",
    "    # A simple polynomial layer that clamps the output between low and high.\n",
    "    def __init__(self, degree, low = -500, high = 500):\n",
    "        super(BoundedPolynomial, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.poly = nn.Linear(degree, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.cat([x ** i for i in range(1, self.degree + 1)], dim=1)\n",
    "        return torch.clamp(self.poly(x), self.low, self.high)\n",
    "\n",
    "# Create the model.\n",
    "class HCE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HCE, self).__init__()\n",
    "        self.mgfc = nn.Linear(NUM_COLORLESS, 1, bias=False)\n",
    "        self.egfc = nn.Linear(NUM_COLORLESS, 1, bias=False)\n",
    "        self.mgcolored = nn.Linear(10, 1, bias = False)\n",
    "        self.egcolored = nn.Linear(10, 1, bias = False)\n",
    "        self.mgpolynomial = BoundedPolynomial(2)\n",
    "        self.egpolynomial = BoundedPolynomial(2)\n",
    "        self.K = 2.0 / 400 # May need tuning, refer to Andrew Grant's Ethereal tuning guide.\n",
    "        self.default = default\n",
    "    \n",
    "    def load_default(self):\n",
    "        # Default is exactly NUM_FEATURES * 2 long, since it contains both the mg and eg values.\n",
    "        self.mgfc.weight.data = self.default[:, :NUM_FEATURES-1].reshape(1, -1)\n",
    "        self.egfc.weight.data = self.default[:, NUM_FEATURES-1:].reshape(1, -1)\n",
    "\n",
    "    def forward(self, phase, colorless, colored):\n",
    "        # Calculate the mg and eg values.\n",
    "        mg = self.mgfc(colorless) * phase\n",
    "        eg = self.egfc(colorless) * (1 - phase)\n",
    "        # Calculate the colored features. First, split the features\n",
    "        wking = colored[:, :10]\n",
    "        bking = colored[:, 10:]\n",
    "        # Get mg and eg term for both colors\n",
    "        wmgk = self.mgcolored(wking) * phase\n",
    "        bmgk = self.mgcolored(bking) * phase\n",
    "        wegk = self.egcolored(wking) * (1 - phase)\n",
    "        begk = self.egcolored(bking) * (1 - phase)\n",
    "        # Pass through the polynomial layer.\n",
    "        wmgk = self.mgpolynomial(wmgk)  \n",
    "        bmgk = self.mgpolynomial(bmgk)\n",
    "        wegk = self.egpolynomial(wegk)\n",
    "        begk = self.egpolynomial(begk)\n",
    "        # Sum the terms.\n",
    "        mg += wmgk - bmgk\n",
    "        eg += wegk - begk\n",
    "        # Calculate the sigmoid.\n",
    "        return 1 / (1 + torch.exp(-self.K * (mg + eg)))\n",
    "\n",
    "# Create the model and load the default values.\n",
    "# model = load_model(\"runs/modelpawnshelter_2047.pt\")\n",
    "model = HCE()\n",
    "# model.load_default()\n",
    "# print(model.default.shape)\n",
    "# print(model.mgfc.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "# Only this extra line of code is required to use oneDNN Graph\n",
    "# torch.jit.enable_onednn_fusion(True)\n",
    "\n",
    "\n",
    "# Create the dataset and the dataloader. Use large batch size to speed up training and avoid overfitting (this is a very simple problem).\n",
    "train_dataset = ChessDataset(phases_train, colorless_train, color_dependent_train, labels_train)\n",
    "test_dataset = ChessDataset(phases_test, colorless_test, color_dependent_test, labels_test)\n",
    "\n",
    "BATCH_SIZE = 16384 * 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=30, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=30, pin_memory=True)\n",
    "\n",
    "# model = load_model(\"runs/modelpawnshelter_13.pt\")\n",
    "# Compile the model.\n",
    "# model = torch.compile(m)\n",
    "# model = HCE()\n",
    "# Find the optimal K value.\n",
    "# K = model.find_optimal_k(train_loader)\n",
    "# print(\"Optimal K value:\", K)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=.001, weight_decay=0.001) # We use a bigger than usual learning rate, since the scale of the weights is much larger.\n",
    "\n",
    "# Create the loss function. The original paper uses MSE.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create the scheduler.\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.95)\n",
    "\n",
    "# Train the model.\n",
    "EPOCHS = 1024\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# Create the tensorboard writer.\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter(log_dir=\"logs\", flush_secs=10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Test) Epoch 2048, Loss: 0.19046588242053986 Time: 18.366507053375244 s\n",
      "(Test) Epoch 2049, Loss: 0.19045430421829224 Time: 17.694855451583862 s\n",
      "(Test) Epoch 2050, Loss: 0.1904429793357849 Time: 17.717244386672974 s\n",
      "(Test) Epoch 2051, Loss: 0.19043231010437012 Time: 17.671803951263428 s\n",
      "(Test) Epoch 2052, Loss: 0.1904216706752777 Time: 17.57187056541443 s\n",
      "(Test) Epoch 2053, Loss: 0.19041132926940918 Time: 18.18381428718567 s\n",
      "(Test) Epoch 2054, Loss: 0.1904008388519287 Time: 18.131683826446533 s\n",
      "(Test) Epoch 2055, Loss: 0.19039058685302734 Time: 17.89490056037903 s\n",
      "(Test) Epoch 2056, Loss: 0.19038096070289612 Time: 18.025938272476196 s\n",
      "(Test) Epoch 2057, Loss: 0.1903712898492813 Time: 18.177820920944214 s\n",
      "(Test) Epoch 2058, Loss: 0.19036176800727844 Time: 17.945281505584717 s\n",
      "(Test) Epoch 2059, Loss: 0.19035232067108154 Time: 18.11404299736023 s\n",
      "(Test) Epoch 2060, Loss: 0.19034293293952942 Time: 18.00236177444458 s\n",
      "(Test) Epoch 2061, Loss: 0.19033369421958923 Time: 18.059348821640015 s\n",
      "(Test) Epoch 2062, Loss: 0.19032439589500427 Time: 18.136505603790283 s\n",
      "(Test) Epoch 2063, Loss: 0.1903150975704193 Time: 17.961714029312134 s\n",
      "(Test) Epoch 2064, Loss: 0.19030635058879852 Time: 17.98085594177246 s\n",
      "(Test) Epoch 2065, Loss: 0.1902976930141449 Time: 18.27790331840515 s\n",
      "(Test) Epoch 2066, Loss: 0.1902892142534256 Time: 17.945488929748535 s\n",
      "(Test) Epoch 2067, Loss: 0.19028082489967346 Time: 18.076006650924683 s\n",
      "(Test) Epoch 2068, Loss: 0.1902725100517273 Time: 18.179412603378296 s\n",
      "(Test) Epoch 2069, Loss: 0.19026421010494232 Time: 17.869565963745117 s\n",
      "(Test) Epoch 2070, Loss: 0.19025593996047974 Time: 17.935252904891968 s\n",
      "(Test) Epoch 2071, Loss: 0.19024768471717834 Time: 17.98635983467102 s\n",
      "(Test) Epoch 2072, Loss: 0.1902398318052292 Time: 17.903342723846436 s\n",
      "(Test) Epoch 2073, Loss: 0.19023214280605316 Time: 17.86142373085022 s\n",
      "(Test) Epoch 2074, Loss: 0.19022437930107117 Time: 17.94640278816223 s\n",
      "(Test) Epoch 2075, Loss: 0.1902167648077011 Time: 18.126947164535522 s\n",
      "(Test) Epoch 2076, Loss: 0.19020909070968628 Time: 18.03683829307556 s\n",
      "(Test) Epoch 2077, Loss: 0.190201535820961 Time: 18.159971475601196 s\n",
      "(Test) Epoch 2078, Loss: 0.19019421935081482 Time: 17.855421543121338 s\n",
      "(Test) Epoch 2079, Loss: 0.19018688797950745 Time: 17.795875549316406 s\n",
      "(Test) Epoch 2080, Loss: 0.19017992913722992 Time: 17.91045045852661 s\n",
      "(Test) Epoch 2081, Loss: 0.19017310440540314 Time: 17.93821096420288 s\n",
      "(Test) Epoch 2082, Loss: 0.19016623497009277 Time: 17.88122582435608 s\n",
      "(Test) Epoch 2083, Loss: 0.19015948474407196 Time: 17.99340319633484 s\n",
      "(Test) Epoch 2084, Loss: 0.19015279412269592 Time: 17.844560861587524 s\n",
      "(Test) Epoch 2085, Loss: 0.1901460587978363 Time: 17.790934562683105 s\n",
      "(Test) Epoch 2086, Loss: 0.19013936817646027 Time: 17.95338726043701 s\n",
      "(Test) Epoch 2087, Loss: 0.19013270735740662 Time: 17.957857847213745 s\n",
      "(Test) Epoch 2088, Loss: 0.1901264637708664 Time: 17.864428520202637 s\n",
      "(Test) Epoch 2089, Loss: 0.19012019038200378 Time: 17.844690561294556 s\n",
      "(Test) Epoch 2090, Loss: 0.19011417031288147 Time: 17.855050563812256 s\n",
      "(Test) Epoch 2091, Loss: 0.19010798633098602 Time: 17.94197964668274 s\n",
      "(Test) Epoch 2092, Loss: 0.19010193645954132 Time: 17.68209671974182 s\n",
      "(Test) Epoch 2093, Loss: 0.1900959461927414 Time: 17.797520399093628 s\n",
      "(Test) Epoch 2094, Loss: 0.19008994102478027 Time: 17.887896060943604 s\n",
      "(Test) Epoch 2095, Loss: 0.19008399546146393 Time: 17.84813666343689 s\n",
      "(Test) Epoch 2096, Loss: 0.19007834792137146 Time: 17.86475706100464 s\n",
      "(Test) Epoch 2097, Loss: 0.19007283449172974 Time: 17.850351333618164 s\n",
      "(Test) Epoch 2098, Loss: 0.19006730616092682 Time: 17.892409801483154 s\n",
      "(Test) Epoch 2099, Loss: 0.1900617778301239 Time: 17.967394828796387 s\n",
      "(Test) Epoch 2100, Loss: 0.19005627930164337 Time: 17.948779821395874 s\n",
      "(Test) Epoch 2101, Loss: 0.19005076587200165 Time: 18.03679895401001 s\n",
      "(Test) Epoch 2102, Loss: 0.19004538655281067 Time: 17.761699199676514 s\n",
      "(Test) Epoch 2103, Loss: 0.1900400072336197 Time: 17.91832423210144 s\n",
      "(Test) Epoch 2104, Loss: 0.19003494083881378 Time: 18.043102741241455 s\n",
      "(Test) Epoch 2105, Loss: 0.19002991914749146 Time: 17.80357789993286 s\n",
      "(Test) Epoch 2106, Loss: 0.1900249719619751 Time: 18.042280197143555 s\n",
      "(Test) Epoch 2107, Loss: 0.19001998007297516 Time: 18.051717281341553 s\n",
      "(Test) Epoch 2108, Loss: 0.19001491367816925 Time: 17.9284770488739 s\n",
      "(Test) Epoch 2109, Loss: 0.1900099217891693 Time: 18.04370427131653 s\n",
      "(Test) Epoch 2110, Loss: 0.1900051385164261 Time: 17.83632493019104 s\n",
      "(Test) Epoch 2111, Loss: 0.19000020623207092 Time: 17.969871997833252 s\n",
      "(Test) Epoch 2112, Loss: 0.18999554216861725 Time: 17.97277522087097 s\n",
      "(Test) Epoch 2113, Loss: 0.18999092280864716 Time: 17.792067289352417 s\n",
      "(Test) Epoch 2114, Loss: 0.18998631834983826 Time: 17.88109278678894 s\n",
      "(Test) Epoch 2115, Loss: 0.18998166918754578 Time: 17.96690058708191 s\n",
      "(Test) Epoch 2116, Loss: 0.1899769902229309 Time: 18.017014026641846 s\n",
      "(Test) Epoch 2117, Loss: 0.1899724155664444 Time: 17.73807978630066 s\n",
      "(Test) Epoch 2118, Loss: 0.18996794521808624 Time: 17.871896743774414 s\n",
      "(Test) Epoch 2119, Loss: 0.1899634152650833 Time: 17.861659288406372 s\n",
      "(Test) Epoch 2120, Loss: 0.18995925784111023 Time: 18.02745032310486 s\n",
      "(Test) Epoch 2121, Loss: 0.18995510041713715 Time: 17.94326138496399 s\n",
      "(Test) Epoch 2122, Loss: 0.18995092809200287 Time: 17.99027991294861 s\n",
      "(Test) Epoch 2123, Loss: 0.1899467557668686 Time: 18.12178611755371 s\n",
      "(Test) Epoch 2124, Loss: 0.18994258344173431 Time: 17.859638452529907 s\n",
      "(Test) Epoch 2125, Loss: 0.18993844091892242 Time: 17.686264514923096 s\n",
      "(Test) Epoch 2126, Loss: 0.18993434309959412 Time: 17.740259647369385 s\n",
      "(Test) Epoch 2127, Loss: 0.1899302750825882 Time: 17.946974754333496 s\n",
      "(Test) Epoch 2128, Loss: 0.1899263709783554 Time: 17.79047417640686 s\n",
      "(Test) Epoch 2129, Loss: 0.18992245197296143 Time: 17.88247776031494 s\n",
      "(Test) Epoch 2130, Loss: 0.18991854786872864 Time: 17.932412147521973 s\n",
      "(Test) Epoch 2131, Loss: 0.18991465866565704 Time: 17.764833211898804 s\n",
      "(Test) Epoch 2132, Loss: 0.18991081416606903 Time: 17.879523992538452 s\n",
      "(Test) Epoch 2133, Loss: 0.1899069845676422 Time: 17.816821575164795 s\n",
      "(Test) Epoch 2134, Loss: 0.1899031698703766 Time: 17.73190951347351 s\n",
      "(Test) Epoch 2135, Loss: 0.18989938497543335 Time: 17.883602142333984 s\n",
      "(Test) Epoch 2136, Loss: 0.1898958832025528 Time: 17.882017135620117 s\n",
      "(Test) Epoch 2137, Loss: 0.18989241123199463 Time: 17.863783836364746 s\n",
      "(Test) Epoch 2138, Loss: 0.18988879024982452 Time: 17.95275616645813 s\n",
      "(Test) Epoch 2139, Loss: 0.18988525867462158 Time: 17.89158344268799 s\n",
      "(Test) Epoch 2140, Loss: 0.18988174200057983 Time: 17.818190097808838 s\n",
      "(Test) Epoch 2141, Loss: 0.18987824022769928 Time: 17.857386589050293 s\n",
      "(Test) Epoch 2142, Loss: 0.18987484276294708 Time: 18.00757098197937 s\n",
      "(Test) Epoch 2143, Loss: 0.1898714154958725 Time: 18.040676593780518 s\n",
      "(Test) Epoch 2144, Loss: 0.18986806273460388 Time: 17.81414270401001 s\n",
      "(Test) Epoch 2145, Loss: 0.18986468017101288 Time: 17.94846749305725 s\n",
      "(Test) Epoch 2146, Loss: 0.18986137211322784 Time: 17.7609121799469 s\n",
      "(Test) Epoch 2147, Loss: 0.1898580938577652 Time: 17.982274532318115 s\n",
      "(Test) Epoch 2148, Loss: 0.18985478579998016 Time: 17.819497108459473 s\n",
      "(Test) Epoch 2149, Loss: 0.1898515373468399 Time: 17.842528104782104 s\n",
      "(Test) Epoch 2150, Loss: 0.18984831869602203 Time: 17.795081615447998 s\n",
      "(Test) Epoch 2151, Loss: 0.18984514474868774 Time: 17.651391983032227 s\n",
      "(Test) Epoch 2152, Loss: 0.18984214961528778 Time: 17.841092824935913 s\n",
      "(Test) Epoch 2153, Loss: 0.18983907997608185 Time: 17.76475715637207 s\n",
      "(Test) Epoch 2154, Loss: 0.18983601033687592 Time: 18.042734146118164 s\n",
      "(Test) Epoch 2155, Loss: 0.18983295559883118 Time: 18.038702487945557 s\n",
      "(Test) Epoch 2156, Loss: 0.18982991576194763 Time: 17.66823101043701 s\n",
      "(Test) Epoch 2157, Loss: 0.18982690572738647 Time: 17.965335845947266 s\n",
      "(Test) Epoch 2158, Loss: 0.18982388079166412 Time: 17.82914662361145 s\n",
      "(Test) Epoch 2159, Loss: 0.18982093036174774 Time: 17.74437117576599 s\n",
      "(Test) Epoch 2160, Loss: 0.18981803953647614 Time: 17.981677532196045 s\n",
      "(Test) Epoch 2161, Loss: 0.18981517851352692 Time: 17.68855309486389 s\n",
      "(Test) Epoch 2162, Loss: 0.1898123174905777 Time: 17.51658797264099 s\n",
      "(Test) Epoch 2163, Loss: 0.18980944156646729 Time: 17.701830625534058 s\n",
      "(Test) Epoch 2164, Loss: 0.1898064911365509 Time: 17.74875569343567 s\n",
      "(Test) Epoch 2165, Loss: 0.1898035705089569 Time: 17.673258066177368 s\n",
      "(Test) Epoch 2166, Loss: 0.1898006796836853 Time: 17.478657245635986 s\n",
      "(Test) Epoch 2167, Loss: 0.18979789316654205 Time: 17.520290851593018 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "for epoch in range(2048, 1024+2048):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_start = time.time()\n",
    "    for i, (phase, colorless, color_dependent, labels) in enumerate(train_loader):\n",
    "        phase, colorless, color_dependent, labels = phase.to(device, non_blocking = True), colorless.to(device, non_blocking = True), color_dependent.to(device, non_blocking = True), labels.to(device, non_blocking = True)\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        y_pred = model(phase, colorless, color_dependent)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        # print(f\"Epoch {epoch}, Batch {i} calculate                \", end=\"\\r\")\n",
    "        loss.backward()\n",
    "        # print(f\"Epoch {epoch}, Batch {i} backward                \", end=\"\\r\")\n",
    "        optimizer.step()\n",
    "        # print(f\"Epoch {epoch}, Batch {i} step                \", end=\"\\r\")\n",
    "        \n",
    "        #if i % 27 == 26:\n",
    "            # writer.add_scalar('Loss/train', running_loss / 8, epoch * len(train_loader) + i)\n",
    "            #print(f\"(Train) Epoch {epoch}, Batch {i}, Loss: {running_loss / 27}                            \", e\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    for i, (phase, colorless, color_dependent, labels) in enumerate(test_loader):\n",
    "        phase, colorless, color_dependent, labels = phase.to(device, non_blocking = True), colorless.to(device, non_blocking = True), color_dependent.to(device, non_blocking = True), labels.to(device, non_blocking = True)\n",
    "        y_pred = model(phase, colorless, color_dependent)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Save the model.\n",
    "    torch.save(model.state_dict(), f\"runs/modelpawnshelter_{epoch}.pt\")\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(f\"(Test) Epoch {epoch}, Loss: {running_loss / len(test_loader)} Time: {epoch_end - epoch_start} s\")\n",
    "\n",
    "    # Save the optimizer.\n",
    "    # torch.save(optimizer.state_dict(), f\"optimizer_{epoch}.pt\")\n",
    "\n",
    "# Save final model.\n",
    "torch.save(model.state_dict(), \"modelSGD.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize(model):\n",
    "    # Visualize the model. The weights are divided in mg and eg.\n",
    "    # The first 5 weights are the material values.\n",
    "    # Then, 64 * 6 weights are the piece square tables.\n",
    "    # Then 9, 14, 15 and 28 piece mobility features.\n",
    "    # Visualize these weights using matplotlib and heatmaps.\n",
    "    weights = model.mgfc.weight.data[0]\n",
    "    # Material values.\n",
    "    material = weights[:5]\n",
    "    # Piece square tables.\n",
    "    pst = weights[5:5+64*6].reshape(6, 64)\n",
    "    # Piece mobility features.\n",
    "    knight_mobility = weights[5+64*6:5+64*6+9]\n",
    "    bishop_mobility = weights[5+64*6+9:5+64*6+9+14]\n",
    "    rook_mobility = weights[5+64*6+9+14:5+64*6+9+14+15]\n",
    "    queen_mobility = weights[5+64*6+9+14+15:5+64*6+9+14+15+28]\n",
    "    # Create the plots. First, plot the psqt. To do this, we need to:\n",
    "    # Add the material values to the pst (excluding the king).\n",
    "    pst[0, :] += material[0]\n",
    "    pst[1, :] += material[1]\n",
    "    pst[2, :] += material[2]\n",
    "    pst[3, :] += material[3]\n",
    "    pst[4, :] += material[4]\n",
    "    # Now plot the pst.\n",
    "    fig, axs = plt.subplots(2, 3)\n",
    "    # Make the figure bigger using figure() method.\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    fig.suptitle(\"Piece Square Tables\")\n",
    "    for i in range(6):\n",
    "        axs[i//3, i%3].imshow(pst[i].reshape(8, 8), cmap='hot', interpolation='nearest')\n",
    "        axs[i//3, i%3].set_title(\"Piece: \" + str(i))\n",
    "        # Add the value text to the plot.\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                axs[i//3, i%3].text(x, y, str(round(pst[i, y*8 + x].item())), color='black', ha='center', va='center')\n",
    "    plt.show()\n",
    "    # Now show the mobility features. For each piece, plot the mobility features using a bar plot.\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.set_size_inches(12.5, 6.5)\n",
    "    fig.suptitle(\"Piece Mobility Features\")\n",
    "    axs[0, 0].bar(range(9), knight_mobility)\n",
    "    axs[0, 0].set_title(\"Knight Mobility\")\n",
    "    axs[0, 1].bar(range(14), bishop_mobility)\n",
    "    axs[0, 1].set_title(\"Bishop Mobility\")\n",
    "    axs[1, 0].bar(range(15), rook_mobility)\n",
    "    axs[1, 0].set_title(\"Rook Mobility\")\n",
    "    axs[1, 1].bar(range(28), queen_mobility)\n",
    "    axs[1, 1].set_title(\"Queen Mobility\")   \n",
    "    # Add the value text to the plot.\n",
    "    for i in range(9):\n",
    "        axs[0, 0].text(i, knight_mobility[i], str(round(knight_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "    for i in range(14):\n",
    "        axs[0, 1].text(i, bishop_mobility[i], str(round(bishop_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "    for i in range(15):\n",
    "        axs[1, 0].text(i, rook_mobility[i], str(round(rook_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "    for i in range(28):\n",
    "        axs[1, 1].text(i, queen_mobility[i], str(round(queen_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "\n",
    "    # Now for eg.\n",
    "    weights = model.egfc.weight.data[0]\n",
    "    # Material values.\n",
    "    material = weights[:5]\n",
    "    # Piece square tables.\n",
    "    pst = weights[5:5+64*6].reshape(6, 64)\n",
    "    # Piece mobility features.\n",
    "    knight_mobility = weights[5+64*6:5+64*6+9]\n",
    "    bishop_mobility = weights[5+64*6+9:5+64*6+9+14]\n",
    "    rook_mobility = weights[5+64*6+9+14:5+64*6+9+14+15]\n",
    "    queen_mobility = weights[5+64*6+9+14+15:5+64*6+9+14+15+28]\n",
    "    # Create the plots. First, plot the psqt. To do this, we need to:\n",
    "    # Add the material values to the pst (excluding the king).\n",
    "    pst[0, :] += material[0]\n",
    "    pst[1, :] += material[1]\n",
    "    pst[2, :] += material[2]\n",
    "    pst[3, :] += material[3]\n",
    "    pst[4, :] += material[4]\n",
    "    # Now plot the pst.\n",
    "    fig, axs = plt.subplots(2, 3)\n",
    "    # Make the figure bigger using figure() method.\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    fig.suptitle(\"Piece Square Tables\")\n",
    "    for i in range(6):\n",
    "        axs[i//3, i%3].imshow(pst[i].reshape(8, 8), cmap='hot', interpolation='nearest')\n",
    "        axs[i//3, i%3].set_title(\"Piece: \" + str(i))\n",
    "        # Add the value text to the plot.\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                axs[i//3, i%3].text(x, y, str(round(pst[i, y*8 + x].item())), color='black', ha='center', va='center')\n",
    "    plt.show()\n",
    "    # Now show the mobility features. For each piece, plot the mobility features using a bar plot.\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.set_size_inches(12.5, 6.5)\n",
    "    fig.suptitle(\"Piece Mobility Features\")\n",
    "    axs[0, 0].bar(range(9), knight_mobility)\n",
    "    axs[0, 0].set_title(\"Knight Mobility\")\n",
    "    axs[0, 1].bar(range(14), bishop_mobility)\n",
    "    axs[0, 1].set_title(\"Bishop Mobility\")\n",
    "    axs[1, 0].bar(range(15), rook_mobility)\n",
    "    axs[1, 0].set_title(\"Rook Mobility\")\n",
    "    axs[1, 1].bar(range(28), queen_mobility)\n",
    "    axs[1, 1].set_title(\"Queen Mobility\")\n",
    "    # Add the value text to the plot.\n",
    "    for i in range(9):\n",
    "        axs[0, 0].text(i, knight_mobility[i], str(round(knight_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "    for i in range(14):\n",
    "        axs[0, 1].text(i, bishop_mobility[i], str(round(bishop_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "    for i in range(15):\n",
    "        axs[1, 0].text(i, rook_mobility[i], str(round(rook_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "    for i in range(28):\n",
    "        axs[1, 1].text(i, queen_mobility[i], str(round(queen_mobility[i].item())), color='black', ha='center', va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "visualize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final weights. Print the weights rounded to 2 decimal places.\n",
    "p = 5+64*6+9+14+15+28\n",
    "newliners = [\n",
    "    5, 5+64*1,5+64*2,5+64*3,5+64*4,5+64*5,5+64*6, 5+64*6+9, 5+64*6+9+14, 5+64*6+9+14+15, 5+64*6+9+14+15+28,\n",
    "    p+1, p+2, p+3, p+4, p+5, p+6, p+7, p+8,\n",
    "    p+15, p+16, p+17, p+18, p+19, p+20\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def psqtprint(numbers, s):# Determine the maximum width of the numbers\n",
    "    max_width = 4\n",
    "    \n",
    "    # Print the numbers with proper formatting\n",
    "    for i in range(0, len(numbers), 8):\n",
    "        row = numbers[i:i+8]\n",
    "        formatted_row = ', '.join(f'{round(num - s):>{max_width}}' for num in row)\n",
    "        print(f'    {formatted_row} ,')\n",
    "\n",
    "\n",
    "weights = model.mgfc.weight.data[0]\n",
    "stuff = []\n",
    "s = []\n",
    "for i in range(len(weights)):\n",
    "    # print(f\"{round(weights[i].item())}\", end=\", \")\n",
    "    s.append(weights[i].item())\n",
    "    if i+1 in newliners:\n",
    "        # print()\n",
    "        stuff.append(s)\n",
    "        s = []\n",
    "stuff.append(s)\n",
    "\n",
    "heads = [\"const Score\",\"constexpr Score\"]\n",
    "\n",
    "names = [\"mgValues\", \"mgPawnTable\", \"mgKnightTable\", \"mgBishopTable\", \"mgRookTable\", \"mgQueenTable\", \"mgKingTable\", \"knightMobMg\", \"bishopMobMg\", \"rookMobMg\", \"queenMobMg\",\n",
    "         \"DOUBLEISOLATEDPENMG\", \"ISOLATEDPENMG\", \"BACKWARDPENMG\", \"DOUBLEDPENMG\", \"SUPPORTEDPHALANXMG\", \"ADVANCABLEPHALANXMG\", \"R_SUPPORTEDPHALANXMG\", \"R_ADVANCABLEPHALANXMG\",\n",
    "         \"passedRankBonusMg\", \"PASSEDPATHBONUSMG\", \"SUPPORTEDPASSERMG\", \"INNERSHELTERMG\", \"OUTERSHELTERMG\", \"BISHOPPAIRMG\", \"ROOKONOPENFILEMG\", \"ROOKONSEMIOPENFILEMG\", \"TEMPOMG\"]\n",
    "\n",
    "for i in range(len(stuff)):\n",
    "    item = stuff[i]    \n",
    "    if len(item)==64:\n",
    "        print(heads[0]+\" \"+names[i]+\" [\"+str(len(item))+\"] = {\\n\")\n",
    "        psqtprint(item, stuff[0][i-1] if i < 6 else 0)\n",
    "        print(\"\\n};\")\n",
    "    elif len(item)>1:\n",
    "        print(heads[0]+\" \"+names[i]+\" [\"+str(len(item))+\"] = {\",end = \"\")\n",
    "        for x in item:\n",
    "            print(round(x),end=\", \")\n",
    "        print(\"};\")\n",
    "    else:\n",
    "        print(heads[1]+\" \"+names[i]+\" = \"+str(round(item[0]))+\";\")\n",
    "        \n",
    "\n",
    "weights = model.egfc.weight.data[0]\n",
    "stuff = []\n",
    "s = []\n",
    "for i in range(len(weights)):\n",
    "    # print(f\"{round(weights[i].item())}\", end=\", \")\n",
    "    s.append(weights[i].item())\n",
    "    if i+1 in newliners:\n",
    "        # print()\n",
    "        stuff.append(s)\n",
    "        s = []\n",
    "stuff.append(s)\n",
    "\n",
    "heads = [\"const Score\",\"constexpr Score\"]\n",
    "\n",
    "nameseg = [\"egValues\", \"egPawnTable\", \"egKnightTable\", \"egBishopTable\", \"egRookTable\", \"egQueenTable\", \"egKingTable\", \"knightMobEg\", \"bishopMobEg\", \"rookMobEg\", \"queenMobEg\",\n",
    "         \"DOUBLEISOLATEDPENEG\", \"ISOLATEDPENEG\", \"BACKWARDPENEG\", \"DOUBLEDPENEG\", \"SUPPORTEDPHALANXEG\", \"ADVANCABLEPHALANXEG\", \"R_SUPPORTEDPHALANXEG\", \"R_ADVANCABLEPHALANXEG\",\n",
    "         \"passedRankBonusEg\", \"PASSEDPATHBONUSEG\", \"SUPPORTEDPASSEREG\", \"INNERSHELTEREG\", \"OUTERSHELTEREG\", \"BISHOPPAIREG\", \"ROOKONOPENFILEEG\", \"ROOKONSEMIOPENFILEEG\", \"TEMPOEG\"]\n",
    "\n",
    "for i in range(len(stuff)):\n",
    "    item = stuff[i]    \n",
    "    if len(item)==64:\n",
    "        print(heads[0]+\" \"+nameseg[i]+\" [\"+str(len(item))+\"] = {\\n\")\n",
    "        psqtprint(item, stuff[0][i-1] if i < 6 else 0)\n",
    "        print(\"\\n};\")\n",
    "    elif len(item)>1:\n",
    "        print(heads[0]+\" \"+nameseg[i]+\" [\"+str(len(item))+\"] = {\",end = \"\")\n",
    "        for x in item:\n",
    "            print(round(x),end=\", \")\n",
    "        print(\"};\")\n",
    "    else:\n",
    "        print(heads[1]+\" \"+nameseg[i]+\" = \"+str(round(item[0]))+\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of numbers\n",
    "numbers = [\n",
    "-71, -40, -35, -4, -12, -3, -9, -85, -9, 32, 30, 14, 30, 38, 31, 0, -3, 28, 42, 55, 50, 66, 48, 8, -7, 22, 45, 57, 59, 57, 40, 12, -24, 18, 33, 58, 53, 41, 30, 1, -27, 2, 18, 30, 28, 21, 10, -5, -43, -8, -7, 5, 9, 1, -12, -35, -80, -61, -38, -26, -38, -28, -53, -89, \n",
    "]\n",
    "\n",
    "# Determine the maximum width of the numbers\n",
    "max_width = max(len(str(num)) for num in numbers)\n",
    "\n",
    "# Print the numbers with proper formatting\n",
    "for i in range(0, len(numbers), 8):\n",
    "    row = numbers[i:i+8]\n",
    "    formatted_row = ', '.join(f'{num:>{max_width}}' for num in row)\n",
    "    print(f'    {formatted_row} ,')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
